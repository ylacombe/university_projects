{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab: nonnegative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix_from_faces(folder='orl_faces/', minidata=False):\n",
    "    # load images\n",
    "    # 400 images of size (112, 92)\n",
    "    M = []\n",
    "    if minidata is True:\n",
    "        nb_subjects = 1\n",
    "    else:\n",
    "        nb_subjects = 40\n",
    "    for subject in range(1, nb_subjects + 1\n",
    "                        ):\n",
    "        for image in range(1, 11):\n",
    "            face = plt.imread(folder + '/s' + str(subject)\n",
    "                              + '/' + str(image) + '.pgm')\n",
    "            M.append(face.ravel())\n",
    "\n",
    "    return np.array(M, dtype=float)\n",
    "\n",
    "def vectorize(W, H):\n",
    "    return np.concatenate((W.ravel(), H.ravel()))\n",
    "\n",
    "def unvectorize_M(W_H, M):\n",
    "    # number of elements in W_H is (n+p)*k where M is of size n x m\n",
    "    # W has the nk first elements\n",
    "    # H has the kp last elements\n",
    "    n, p = M.shape\n",
    "    k = W_H.shape[0] // (n + p)\n",
    "    W = W_H[:n * k].reshape((n, k))\n",
    "    H = W_H[n * k:].reshape((k, p))\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Download the database at https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
    "Uncompress the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small data to test the algorithm\n",
    "M = build_matrix_from_faces(folder='orl_faces/', minidata=True)\n",
    "def unvectorize(W_H): return unvectorize_M(W_H, M)\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To see the first face:\n",
    "plt.imshow(M[0].reshape((112, 92)), cmap='gray'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 images in the data set, each image with 10304 pixels.\n"
     ]
    }
   ],
   "source": [
    "# Full data\n",
    "M = build_matrix_from_faces(folder='orl_faces/', minidata=False)\n",
    "def unvectorize(W_H): return unvectorize_M(W_H, M)\n",
    "k = 38\n",
    "\n",
    "n,p = M.shape\n",
    "\n",
    "print(\"There are \" + str(n) + \" images in the data set, each image with \" + str (p) + \" pixels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1\n",
    "Supposons $ n = p = 1 $ et posons $f(x,y) = \\frac{1}{2} \\cdot (M - x\\cdot y)^2$.\n",
    "\n",
    "La matrice hessienne associée à f est :\n",
    "\n",
    "$$ H = \\frac{1}{2} \\cdot  \\begin{pmatrix}\n",
    "2y^2 & 4xy -2M\\\\\n",
    "4xy -2M & 2x^2\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "\n",
    "Avec $ y = 1 $, le déterminant (multiplié par 4 pour enlever la fraction) de la matrice est :\n",
    "\n",
    "$$ g(x) = -12x^2 -4M^2 + 16Mx $$\n",
    "\n",
    "Or, $$g(x) \\xrightarrow{x \\xrightarrow{} \\infty} -\\infty < 0 $$\n",
    "Nous avons donc qu'à partir d'un certain réel, le déterminant est strictement négatif, c'est à dire que les valeurs propres sont de signes opposées. Ainsi la matrice n'est pas positive et donc f n'est pas convexe.\n",
    "\n",
    "## Question 3.2\n",
    "\n",
    "Posons encore $$ f(W,H) =   \\frac{1}{2np}  {{\\left\\|M - WH\\right\\|}_F}^2 = \\frac{1}{2np} \\cdot  tr((M -WH)\\cdot {(M-WH)}^T) $$\n",
    "\n",
    "Alors, $$ 2np\\cdot f(W+w, H+h) = {{\\left\\|M - WH\\right\\|}_F}^2 -2<M-WH,wH+Wh> + {{\\left\\|wH - Wh\\right\\|}_F}^2 $$\n",
    "\n",
    "En posant successivement $w=0$ et $h=0$, nous trouvons les gradients par rapport à $H$ puis à $W$.\n",
    "\n",
    "D'où $grad(f(W,H)) =-\\frac{1}{np}\\cdot  \\begin{pmatrix}\n",
    "(M-WH)\\cdot H^T\\\\W^T\\cdot (M-WH)\n",
    "  &\n",
    "\\end{pmatrix} $\n",
    "\n",
    "\n",
    "## Question 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0, S, H0 = scipy.sparse.linalg.svds(M, k)\n",
    "\n",
    "W0 = np.maximum(0, W0 * np.sqrt(S))\n",
    "H0 = np.maximum(0,(H0.T * np.sqrt(S)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se rapproche d'un produit $WH = M$ en partant d'un cas où l'on est positivement sûr de cet égalité (à savoir $M = W_0SH_0$). \n",
    "\n",
    "On est sûr que $W_0$ et $H_0$ ont des coefficients positifs avec la fonction ``np.maximum``. On a donc une heuristique, qui espérons-le, permet de se rapprocher de la solution de manière plus rapide.\n",
    "\n",
    "On aurait pu également choisir $W_0 = W_0 \\cdot S$ et $H_0$ tel quel ou $H_0 = H_0 \\cdot S$ avec $W_0$ tel quel.\n",
    "On aurait pu de même choisir $W_0$ et $H_0$ de manière totalement arbitraire.\n",
    "\n",
    "## Question 4.2\n",
    "\n",
    "On a déjà que le gradient de  $g$ est ainsi (voir question 3.2):\n",
    " $grad(g(W)) =-\\frac{1}{np}\\cdot (M-WH)\\cdot H^T $\n",
    " \n",
    "D'où la matrice Hessienne est :\n",
    "$$H_g = \\frac{1}{np} \\cdot {(HH^T)}^T =\\frac{1}{np} \\cdot HH^T $$\n",
    "C'est bien une matrice positive donc $g$ est convexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(W):\n",
    "    return np.trace(np.dot(M-W@H0, (M-W@H0).T))/(2*n*p)\n",
    "#    return 1/(2*n*p)* np.linalg.norm(M-W@H0,'fro')**2\n",
    "\n",
    "def grad_g(W):\n",
    "    return (W.dot(H0)-M).dot(np.transpose(H0))/(n*p)\n",
    "\n",
    "def g_unvec(W_unvec):\n",
    "    return 1/(2*n*p)* np.linalg.norm(M.reshape(-1)-(W_unvec.reshape((n,k))@H0).reshape(-1))**2\n",
    "\n",
    "def grad_g_unvec(W_unvec):\n",
    "    return ((M -(W_unvec.reshape((n,k)))@H0)@H0.T).reshape(-1)/(-n*p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3\n",
    "Le résultat retourné pour scipy.optimize.check_grad(g_unvec, grad_g_unvec, np.ones((n.k)))\n",
    "est 0.006656803752079777.\n",
    "Ce résultat étant long à obtenir, nous commentons le code à faire. Le résultat étant proche de 0, on peut supposer que les deux fonctions codées soient bonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy.optimize.check_grad(g_unvec, grad_g_unvec, np.ones((n*k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.4\n",
    "Renommons $\\gamma \\iota \\mathbb{R}_+$ en $func$.\n",
    "Remarquons que $prox_{func\n",
    "}(y) = argmin_{x \\in \\mathbb{R}}( func(x) + {{\\left\\|x - y\\right\\|}_2}^2 ) = argmin_{x \\in \\mathbb{R}_+}( func(x) + {{\\left\\|x - y\\right\\|}_2}^2 ) $, car $func$ est infini sur l'espace des réel négatifs.\n",
    "\n",
    "Si $ y \\geq 0 $, on a donc  $prox_{func\n",
    "}(y) = argmin_{x \\in \\mathbb{R}_+}({{\\left\\|x - y\\right\\|}_2}^2 ) = y$ car ce minimum est atteint en 0 par y.\n",
    "\n",
    "Si $ y \\leq 0 $, $prox_{func\n",
    "}(y) = argmin_{x \\in \\mathbb{R}_+}({{\\left\\|x - y\\right\\|}_2}^2 ) = 0$.\n",
    "\n",
    "La fonction est donc bien la projection sur les réels positifs.\n",
    "\n",
    "## Question 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(u):\n",
    "    if u>=0:\n",
    "        return u\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def projected_gradient_method(val_g, grad_g, W0, gamma, N):\n",
    "    x = W0\n",
    "    L = [val_g(W0)]\n",
    "    for i in range(N):\n",
    "        intermediaire = x-gamma*grad_g(x)\n",
    "\n",
    "        x = np.array([[prox(i) for i in j] for j in intermediaire])\n",
    "        L+=[val_g(x)]\n",
    "    return x,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode avec gamma = 1/constante_de_lipschitz (3.4555429731728355e-06) : [867.6350229602607, 867.6348753266026, 867.63472769301, 867.6345800594828, 867.634432426021, 867.6342847926244, 867.6341371592933, 867.6339895260279, 867.6338418928275, 867.6336942596927, 867.6335466266231, 867.6333989936192, 867.6332513606806, 867.6331037278072, 867.6329560949994, 867.6328084622569, 867.6326608295799, 867.6325131969681, 867.6323655644219, 867.6322179319411, 867.6320702995256, 867.6319226671756, 867.631775034891, 867.6316274026716, 867.6314797705178, 867.6313321384295, 867.6311845064063, 867.6310368744487, 867.6308892425563, 867.6307416107295, 867.6305939789681, 867.6304463472719, 867.6302987156413, 867.630151084076, 867.6300034525761, 867.6298558211416, 867.6297081897727, 867.6295605584689, 867.6294129272306, 867.6292652960578, 867.6291176649505, 867.6289700339082, 867.6288224029315, 867.6286747720203, 867.6285271411745, 867.628379510394, 867.6282318796789, 867.6280842490293, 867.6279366184449, 867.6277889879261, 867.6276413574726, 867.6274937270846, 867.6273460967619, 867.6271984665045, 867.6270508363127, 867.6269032061862, 867.6267555761252, 867.6266079461294, 867.626460316199, 867.6263126863342, 867.6261650565348, 867.6260174268007, 867.6258697971318, 867.6257221675286, 867.6255745379907, 867.6254269085181, 867.6252792791109, 867.6251316497695, 867.6249840204931, 867.6248363912821, 867.6246887621367, 867.6245411330565, 867.6243935040418, 867.6242458750926, 867.6240982462086, 867.6239506173899, 867.6238029886368, 867.6236553599491, 867.6235077313268, 867.6233601027698, 867.6232124742784, 867.6230648458521, 867.6229172174915, 867.6227695891961, 867.6226219609661, 867.6224743328015, 867.6223267047023, 867.6221790766685, 867.6220314487002, 867.6218838207973, 867.6217361929597, 867.6215885651876, 867.6214409374808, 867.6212933098394, 867.6211456822635, 867.6209980547529, 867.6208504273077, 867.620702799928, 867.6205551726136, 867.6204075453645, 867.620259918181]\n",
      "\n",
      "\n",
      "Méthode avec gamma = 1 : [867.6350229602607, 826.2863462671129, 790.0867502067483, 758.3890803382805, 730.6306675207239, 706.3178484854432, 685.0182929686885, 666.3542112345589, 649.9961241284118, 635.6578759525078, 623.0878284111963, 612.0655710540686, 602.3986357121174, 593.9188032011656, 586.4783820013846, 579.9485327272938, 574.2165775422345, 569.1830522015103, 564.7612887196668, 560.8750861685357, 557.4574760396638, 554.4504594394527, 551.8032190963237, 549.4713585571662, 547.415681309958, 545.6019909334066, 544.0002090016742, 542.5841976951947, 541.3309498634134, 540.2202826068936, 539.234554150441, 538.3583589475197, 537.5780301474301, 536.8817835969504, 536.2591921974712, 535.7011618370898, 535.1998155121903, 534.7480889534808, 534.339839442358, 533.9698517126315, 533.6333970312195, 533.3263086473589, 533.0452123160366, 532.7867548458732, 532.5481096031955, 532.3268563353159, 532.1209451180816, 531.9284271406601, 531.7476984487115, 531.5774305906799, 531.4164008065482, 531.2634379341669, 531.1175260840937, 530.9779108855527, 530.8437760281445, 530.7145054644667, 530.5895678558709, 530.4684583295776, 530.3507716485286, 530.236045148557, 530.1239923940466, 530.0142879898259, 529.9066780071886, 529.8009603517272, 529.6969620597899, 529.5944971670962, 529.4933723878506, 529.393489663572, 529.2947075772466, 529.1969137209858, 529.1000422421695, 529.0040219522693, 528.9087847047444, 528.814336643305, 528.7205901468942, 528.62744812094, 528.5348817687424, 528.4428739187879, 528.3514015891698, 528.2604002880781, 528.1698604755433, 528.0798026206978, 527.9901937116741, 527.9010133839729, 527.8122759966228, 527.7238909742298, 527.635870401227, 527.5481973775346, 527.4608748403136, 527.3738714139995, 527.2872166439739, 527.2008549891042, 527.1147831802563, 527.029001131535, 526.9435125970634, 526.8583114318604, 526.7734422980036, 526.6888625824629, 526.6045792742818, 526.5205725920455, 526.4368213146104]\n"
     ]
    }
   ],
   "source": [
    "lipschitz_cst = np.linalg.norm(H0.T@H0, 'fro')\n",
    "\n",
    "y, L_lip = projected_gradient_method(g,grad_g,W0,1/lipschitz_cst,100)\n",
    "\n",
    "y, L = projected_gradient_method(g,grad_g,W0,1,100)\n",
    "\n",
    "print(\"Méthode avec gamma = 1/constante_de_lipschitz (\"+ str(1/lipschitz_cst)+\") :\", L_lip)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Méthode avec gamma = 1 :\", L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que la valeur du pas est très faible, ce qui donne un algorithme très lent à converger. En revanche, en montant $\\gamma$ à 1, l'algorithme avance bien plus vite. Il faut donc trouver une valeur optimale de $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method_free(val_g, grad_g, W0, N, lip):\n",
    "    x = W0\n",
    "\n",
    "    L = [val_g(W0)]\n",
    "    gamma = lip\n",
    "    for i in range(N):\n",
    "        b = 2*gamma\n",
    "        a = .5\n",
    "        \n",
    "        x_plus_gamma = x - gamma * grad_g(x)\n",
    "        while val_g(x_plus_gamma) > val_g(x) + np.vdot(grad_g(x), x_plus_gamma - x) + 1/(2*gamma) * np.linalg.norm(x - x_plus_gamma) ** 2:\n",
    "            gamma *= a\n",
    "            x_plus_gamma = x - gamma * grad_g(x)\n",
    "        intermediaire = x-gamma*grad_g(x)\n",
    "\n",
    "        x = np.array([[prox(k) for k in j] for j in intermediaire])\n",
    "        L+=[val_g(x)]\n",
    "\n",
    "    return x, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode avec la Taylor-based line-search : [867.6350229602607, 597.5050684472858, 546.0555552989091, 535.5635039213015, 532.8253330301245, 531.5768434521453, 530.6319705412031, 529.7664013854667, 528.93578382914, 528.1317786668228, 527.352592006273, 526.5957097987831, 525.8602969350624, 525.1462150994605, 524.4522759991062, 523.7785320419217, 523.1225354365297, 522.4836482193402, 521.8611604064272, 521.2554079360447, 520.6655227417407, 520.0911183359921, 519.5307212816286, 518.984357624683, 518.4514711330365, 517.9313739612234, 517.4233882889988, 516.9274059640131, 516.4419910580434, 515.9668997585226, 515.5023702485767, 515.0475571462432, 514.6022301621076, 514.1664691770814, 513.7402526800979, 513.3227207682014, 512.9135867421617, 512.5126398243514, 512.119877996329, 511.7351655674286, 511.3582576974474, 510.9887029152731, 510.6263946073539, 510.2707927744759, 509.9216955832692, 509.57899802177116, 509.24239788463535, 508.91277009275547, 508.58915961412566, 508.2712966857452, 507.95903311435757, 507.6520501678065, 507.35036951223657, 507.05379307668335, 506.7622216813904, 506.47549172556575, 506.193640894024, 505.9163514874426, 505.6437097850627, 505.3757365832671, 505.1122807113253, 504.85304804345054, 504.59801401697155, 504.34710246400397, 504.10015104131463, 503.8571293535972, 503.6177215284507, 503.38191680467105, 503.15004417285644, 502.92210898173266, 502.69805077051586, 502.4774196616509, 502.26024371947676, 502.04649876457, 501.8360231050747, 501.6285946251588, 501.42411948007657, 501.22285316560374, 501.0245164631732, 500.8289779454873, 500.6362466347251, 500.44624412942807, 500.2589245360319, 500.07422874441136, 499.89209737114084, 499.7124490862023, 499.53534034914725, 499.3606576157228, 499.1883151102698, 499.0182896836122, 498.85079419064976, 498.6856057058363, 498.5225913481059, 498.3618270811117, 498.2031743729016, 498.04658681669616, 497.8920568510096, 497.7396860639, 497.58933833319924, 497.4409091855554, 497.2943687459938]\n"
     ]
    }
   ],
   "source": [
    "y, L_lip = projected_gradient_method_free(g,grad_g,W0,100,lipschitz_cst)\n",
    "\n",
    "print(\"Méthode avec la Taylor-based line-search :\", L_lip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que l'algorithme converge en moins d'itérations. Toutefois l'algorithme est plus lent, ce qui est dû à la recherche du pas gamma à chaque itération.\n",
    "\n",
    "## Question 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W,H):\n",
    "    return np.linalg.norm(M-W@H, 'fro')**2/(2*n*p)\n",
    "\n",
    "def grad_f(W,H):\n",
    "    return [(M-W@H)@H.T/(-n*p), W.T@(M-W@H)/(-n*p)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode avec gamma fixe (ici $\\gamma = \\frac{1}{L_0}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_W(val_g, grad_g, WH, N):\n",
    "    x = WH[0]\n",
    "    y = WH[1] \n",
    "    \n",
    "    L = [val_g(x,y)]\n",
    "    for i in range(N):\n",
    "        gamma =  1/np.linalg.norm(y.T@y, 'fro')\n",
    "\n",
    "        grad = grad_g(x,y)\n",
    "        x_plus_gamma = x - gamma * grad[0]\n",
    "        y_plus_gamma = y - gamma * grad[1]\n",
    "\n",
    "        x = np.array([[prox(k) for k in j] for j in x_plus_gamma])\n",
    "        y = np.array([[prox(k) for k in j] for j in y_plus_gamma])\n",
    "        \n",
    "        L+=[val_g(x,y)]\n",
    "    return [x,y], L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "debut_projected1 = time()\n",
    "y, L_a = projected_gradient_W(f, grad_f, np.array([W0, H0]), 100)\n",
    "fin_projected1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[867.6350229602598, 867.6347132301219, 867.6344035002503, 867.6340937706415, 867.633784041298, 867.6334743122233, 867.6331645834115, 867.6328548548654, 867.6325451265843, 867.6322353985712, 867.6319256708207, 867.6316159433355, 867.6313062161167, 867.630996489163, 867.6306867624758, 867.6303770360538, 867.6300673098975, 867.6297575840055, 867.6294478583778, 867.6291381330184, 867.6288284079229, 867.6285186830911, 867.6282089585279, 867.6278992342286, 867.6275895101952, 867.6272797864262, 867.6269700629243, 867.626660339687, 867.626350616714, 867.6260408940091, 867.6257311715659, 867.625421449389, 867.6251117274818, 867.6248020058356, 867.6244922844578, 867.6241825633423, 867.6238728424937, 867.6235631219107, 867.6232534015934, 867.6229436815404, 867.6226339617523, 867.6223242422315, 867.6220145229768, 867.6217048039847, 867.6213950852585, 867.6210853667997, 867.6207756486053, 867.6204659306766, 867.6201562130135, 867.6198464956154, 867.6195367784827, 867.6192270616156, 867.6189173450117, 867.6186076286758, 867.6182979126063, 867.6179881967995, 867.6176784812598, 867.6173687659842, 867.6170590509746, 867.6167493362303, 867.6164396217531, 867.6161299075384, 867.6158201935918, 867.6155104799095, 867.6152007664916, 867.6148910533402, 867.614581340454, 867.614271627834, 867.6139619154777, 867.6136522033901, 867.6133424915649, 867.6130327800062, 867.6127230687109, 867.6124133576828, 867.6121036469206, 867.6117939364217, 867.6114842261916, 867.6111745162253, 867.6108648065242, 867.6105550970877, 867.6102453879183, 867.6099356790136, 867.6096259703716, 867.6093162619985, 867.6090065538896, 867.6086968460468, 867.6083871384693, 867.6080774311567, 867.6077677241085, 867.6074580173279, 867.6071483108094, 867.6068386045608, 867.6065288985748, 867.6062191928557, 867.6059094873997, 867.6055997822111, 867.605290077289, 867.6049803726291, 867.6046706682364, 867.6043609641081, 867.6040512602461]\n",
      "\n",
      "\n",
      "Temps d'exécution de la méthode projected avec gamma imposé à 1/lipschitz : 106.0693690776825\n"
     ]
    }
   ],
   "source": [
    "print(L_a)\n",
    "print(\"\\n\")\n",
    "print(\"Temps d'exécution de la méthode projected avec gamma imposé à 1/lipschitz :\", fin_projected1 - debut_projected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On arrive à un coût de $867.6$ après 100 itérations en prenant à chaque étape un pas $ \\gamma = \\frac{1}{L_0} = \\frac{1}{||(H_O)^TH_0||_F}$. Le pas est beaucoup trop faible pour donner un résultat satisfaisant en un temps raisonnable. Néanmoins, nous ne sommes pas assurés que la fonction objectif soit lipschitzienne (en tout cas, nous ne sommes pas arrivés à déterminer la constante de Lipschitz de la fonction ``f(W, H)``). Nous ne savons si la fonction converge bien avec le $\\gamma$ non fixe que nous avons choisi et nous n'avons pas la main sur une vitesse de convergence.\n",
    "\n",
    "On essaie maintenant la méthode avec *line search*, comme suggéré par l'exercice. Nous ne savons pas comment fixé le premier pas $\\gamma_0$, nous le fixons ici à $\\frac{1}{L_0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_W_method_free(val_g, grad_g, W, H, N):\n",
    "    x = W\n",
    "    y = H \n",
    "    \n",
    "    L = [val_g(x,y)]\n",
    "    \n",
    "    lipschitz_cst = np.linalg.norm(y.T@y, 'fro')\n",
    "    gamma = lipschitz_cst\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        b = 2*gamma\n",
    "        a = .5\n",
    "        grad = grad_g(x,y)\n",
    "        x_plus_gamma = x - gamma * grad[0]\n",
    "        y_plus_gamma = y - gamma * grad[1]\n",
    "                \n",
    "        while val_g(x_plus_gamma, y_plus_gamma) > val_g(x,y) - gamma * (np.trace(np.dot(grad[0].T, grad[0])) + np.trace(np.dot(grad[1].T, grad[1]))) + 1/(2*gamma) * (np.linalg.norm(gamma*grad[0], 'fro')**2 + np.linalg.norm(gamma*grad[1], 'fro')**2) :                           \n",
    "            gamma *= a\n",
    "            x_plus_gamma = x - gamma * grad[0]\n",
    "            y_plus_gamma = y - gamma * grad[1]\n",
    "\n",
    "        x = np.array([[prox(k) for k in j] for j in x_plus_gamma])\n",
    "        y = np.array([[prox(k) for k in j] for j in y_plus_gamma])\n",
    "\n",
    "        L+=[val_g(x,y)]\n",
    "        \n",
    "    return [x,y], L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "debut_projected2 = time()\n",
    "y, L_b = projected_gradient_W_method_free(f, grad_f, W0, H0, 100)\n",
    "fin_projected2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[867.6350229602598, 584.5832848298361, 522.0751987243319, 501.49945265464333, 492.3017263611699, 486.9888793462459, 483.2788941613555, 480.33510467578395, 477.7946615307879, 475.4804943158429, 473.29920271260653, 471.19958003256954, 469.1530914341907, 467.14390374906355, 465.162855213977, 463.20464238236445, 461.26644107339985, 459.3466293756908, 457.44429081147973, 455.5589452816401, 453.69034372158814, 451.8384750561858, 450.0033894884321, 448.1848676791891, 446.3828690255691, 444.59738548509097, 442.82844079477434, 441.0759825233914, 439.3398527623801, 437.6201389201931, 435.9165740436995, 434.22907863378174, 432.5575361466345, 430.90198342218406, 429.2620883676003, 427.63792351948126, 426.02936754150113, 424.4360943042163, 422.8578916479454, 421.29454877129456, 419.7458782185908, 418.2117604173033, 416.6920905814368, 415.1866679260802, 413.69544250401106, 412.2180346988885, 410.7543234689598, 409.30428923745967, 407.86761284185644, 406.4440190623542, 405.03328633371603, 403.635295382188, 402.24990298983886, 400.87693985676356, 399.5163753613576, 398.167974697531, 396.83140523743384, 395.5065547447958, 394.193281165443, 392.89135795343407, 391.6006493500462, 390.3210048500821, 389.0523365521251, 387.79446508245917, 386.54723359079355, 385.31057935132384, 384.0843120426871, 382.86828797118153, 381.66240032414765, 380.46655253593747, 379.2804894969292, 378.10416676154233, 376.9375026925249, 375.7802652430118, 374.6324165235596, 373.4938923386747, 372.36453523422574, 371.2443802763793, 370.1332052451592, 369.03095315189825, 367.93746122611844, 366.8526322991298, 365.7765663721604, 364.70901917804764, 363.6501371126399, 362.59968600074353, 361.55752971457434, 360.5236678430308, 359.4979857414617, 358.4802878035225, 357.47051850667873, 356.46861539156964, 355.47458677180475, 354.48836657394406, 353.5098324612524, 352.5389245353141, 351.5756062931783, 350.61982236950615, 349.6716267478713, 348.7308553380587, 347.7974019004838]\n",
      "\n",
      "\n",
      "Temps d'exécution de la méthode projected avec recherche de gamma : 129.30510926246643\n"
     ]
    }
   ],
   "source": [
    "print(L_b)\n",
    "print(\"\\n\")\n",
    "print(\"Temps d'exécution de la méthode projected avec recherche de gamma :\", fin_projected2 - debut_projected2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est très bon pour cette méthode. En un peu plus de deux minutes et après 100 itérations, on arrive à un coût inférieur à $348$.\n",
    "\n",
    "## Question 6.2\n",
    "\n",
    "La valeur de la fonction objective à l'étape n est $f_n = f(W_n,H_n)$.\n",
    "Or par définition de l'argmin, la première ligne de l'itération assure que :\n",
    "$$f(W_{n+1},H_n) \\leq f_n$$\n",
    "Et la deuxième ligne que :\n",
    "$$ f_{n+1} = f(W_{n+1},H_{n+1}) \\leq f(W_{n+1},H_n) $$\n",
    "D'où $(f_n)$ est une suite décroissante (qui est positive) donc qui converge.\n",
    "\n",
    "## Question 6.3 et question 6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_method(M, W0, H0, gamma, N_global, N_state):\n",
    "    n,p = M.shape\n",
    "    \n",
    "    H_tild = H0\n",
    "    W_tild = W0\n",
    "    \n",
    "    def g_alternate_H(W):\n",
    "        return f(W,H_tild)\n",
    "    def grad_g_alternate_H(W):\n",
    "        return (W.dot(H_tild)-M).dot(np.transpose(H_tild))/(n*p)\n",
    "    \n",
    "    L = []\n",
    "    \n",
    "    for j in range(N_global):\n",
    "        W_tild, l = projected_gradient_method(g_alternate_H, grad_g_alternate_H , W_tild, gamma, N_state)\n",
    "        L += l\n",
    "\n",
    "        def g_alternate_W(H):\n",
    "            return f(W_tild,H)\n",
    "        def grad_g_alternate_W(H):\n",
    "            return np.transpose(W_tild).dot(W_tild.dot(H) - M)/(n*p)\n",
    "        \n",
    "        H_tild, l = projected_gradient_method(g_alternate_W, grad_g_alternate_W, H_tild, gamma, N_state)\n",
    "        \n",
    "        L+= l\n",
    "        \n",
    "        def g_alternate_H(W):\n",
    "            return f(W,H_tild)\n",
    "        def grad_g_alternate_H(W):\n",
    "            return (W.dot(H_tild)-M).dot(np.transpose(H_tild))/(n*p)\n",
    "        \n",
    "    return W_tild,H_tild,L       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1/np.linalg.norm(H0.T@H0, 'fro')\n",
    "debut_alternate1 = time()\n",
    "W_alternate, H_alternate, L = alternate_method(M,W0,H0, gamma, 10,5)\n",
    "fin_alternate1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[867.6350229602598, 867.6348753266044, 867.6347276930102, 867.6345800594826, 867.6344324260198, 867.6342847926254, 867.6342847926254, 867.6341226964844, 867.6339606004213, 867.6337985044273, 867.6336364085064, 867.6334743126573, 867.6334743126573, 867.6333266797061, 867.6331790468201, 867.6330314139992, 867.6328837812448, 867.6327361485558, 867.6327361485558, 867.6325740531598, 867.6324119578359, 867.6322498625855, 867.6320877674059, 867.631925672299, 867.631925672299, 867.6317780400545, 867.6316304078775, 867.631482775764, 867.6313351437154, 867.6311875117333, 867.6311875117333, 867.6310254170801, 867.6308633224988, 867.6307012279888, 867.6305391335519, 867.6303770391875, 867.6303770391875, 867.630229407652, 867.6300817761801, 867.6299341447742, 867.629786513433, 867.6296388821589, 867.6296388821589, 867.6294767882466, 867.629314694407, 867.6291526006406, 867.6289905069464, 867.6288284133229, 867.6288284133229, 867.6286807824923, 867.6285331517291, 867.6283855210304, 867.6282378903969, 867.6280902598285, 867.6280902598285, 867.6279281666592, 867.6277660735619, 867.6276039805359, 867.6274418875845, 867.6272797947033, 867.6272797947033, 867.6271321645827, 867.6269845345238, 867.6268369045309, 867.626689274606, 867.6265416447445, 867.6265416447445, 867.6263795523172, 867.6262174599618, 867.6260553676793, 867.6258932754697, 867.6257311833319, 867.6257311833319, 867.6255835539166, 867.6254359245654, 867.6252882952806, 867.6251406660618, 867.6249930369079, 867.6249930369079, 867.6248309452218, 867.6246688536098, 867.6245067620704, 867.6243446706009, 867.6241825792052, 867.6241825792052, 867.6240349504967, 867.6238873218537, 867.6237396932753, 867.6235920647628, 867.6234444363162, 867.6234444363162, 867.6232823453722, 867.6231202545018, 867.6229581637034, 867.622796072978, 867.6226339823256, 867.6226339823256, 867.6224863543237, 867.6223387263886, 867.6221910985163, 867.622043470712, 867.6218958429707, 867.6218958429707, 867.6217337527713, 867.6215716626418, 867.6214095725849, 867.621247482602, 867.6210853926915, 867.6210853926915, 867.6209377653972, 867.6207901381665, 867.620642511004, 867.6204948839063, 867.6203472568716, 867.6203472568716, 867.6201851674145, 867.6200230780272, 867.619860988713, 867.6196988994712, 867.6195368103023]\n",
      "\n",
      "\n",
      "Temps d'exécution de la méthode alternate avec gamma imposé à 1/constante_de_lipschitz : 19.240463495254517\n"
     ]
    }
   ],
   "source": [
    "print(L)\n",
    "print(\"\\n\")\n",
    "print(\"Temps d'exécution de la méthode alternate avec gamma imposé à 1/constante_de_lipschitz :\", fin_alternate1 - debut_alternate1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a le même problème qu'à la question 6.1, on arrive seulement à $867.6195368103023$. \n",
    "\n",
    "Le pas $\\gamma$ choisi est beaucoup trop faible. Il faudrait prendre un pas imposé, comme $\\gamma = 1$, auquel cas on obtiendrait un meilleur résultat ($471.2773581587625$ pour cet exemple).\n",
    "\n",
    "Ce résultat est toutefois moins bon que ce qu'on avait trouvé précédemment avec la méthode *line search*. On applique donc cette méthode :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_method_free(M, W0, H0, N_global, N_state):\n",
    "    n, p = M.shape\n",
    "    \n",
    "    H_tild = H0\n",
    "    W_tild = W0\n",
    "    \n",
    "    def g_alternate_H(W):\n",
    "        return f(W,H_tild)\n",
    "    def grad_g_alternate_H(W):\n",
    "        return (W.dot(H_tild)-M).dot(np.transpose(H_tild))/(n*p)\n",
    "    \n",
    "    L = []\n",
    "    \n",
    "    for j in range(N_global):\n",
    "        W_tild, l = projected_gradient_method_free(g_alternate_H, grad_g_alternate_H , W_tild, N_state, np.linalg.norm(H_tild.T@H_tild, 'fro'))\n",
    "        L += l\n",
    "\n",
    "        def g_alternate_W(H):\n",
    "            return f(W_tild,H)\n",
    "        def grad_g_alternate_W(H):\n",
    "            return np.transpose(W_tild).dot(W_tild.dot(H) - M)/(n*p)\n",
    "        \n",
    "        H_tild, l = projected_gradient_method_free(g_alternate_W, grad_g_alternate_W, H_tild, N_state, np.linalg.norm(W_tild@W_tild.T, 'fro'))\n",
    "        \n",
    "        L+= l\n",
    "        \n",
    "        def g_alternate_H(W):\n",
    "            return f(W,H_tild)\n",
    "        def grad_g_alternate_H(W):\n",
    "            return (W.dot(H_tild)-M).dot(np.transpose(H_tild))/(n*p)\n",
    "        \n",
    "    return W_tild, H_tild, L       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "debut_alternate2 = time()\n",
    "W_alternate, H_alternate, L_c = alternate_method_free(M,W0,H0, 10, 10)\n",
    "fin_alternate2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[867.6350229602598, 597.505068447285, 546.055555298909, 535.5635039213022, 532.8253330301242, 531.5768434521458, 530.6319705412031, 529.7664013854667, 528.9357838291397, 528.1317786668222, 527.3525920062726, 527.3525920062726, 487.5838124778775, 477.7586346849457, 473.8920803802056, 471.32632117579107, 469.11212610689614, 467.04480358855454, 465.0745182362635, 463.18475738230893, 461.3679376206291, 459.61892914105937, 459.61892914105937, 450.3628394638088, 438.55678805561695, 435.8172007985179, 433.60674550665317, 431.6208970331273, 429.8140967110323, 428.15958474943244, 426.63634006328044, 425.2263559113618, 423.9149952660259, 423.9149952660259, 412.7394554459156, 408.4242826559246, 404.63496991535334, 401.19719038478286, 398.06050577088985, 395.18635868352044, 392.5427606786255, 390.10249122999534, 387.8427670497, 385.7441239362396, 385.7441239362396, 379.9251659613424, 374.8592555716607, 372.7270189382234, 370.89184483222823, 369.2623222265505, 367.79922751255276, 366.47273694871274, 365.26108318200306, 364.1470048130908, 363.1154688942442, 363.1154688942442, 356.4843098463487, 353.4690268846731, 350.80866727021214, 348.3866938412465, 346.17061409378675, 344.1364225671492, 342.26368115380353, 340.5350593244994, 338.9356052947752, 337.45233095333043, 337.45233095333043, 333.0144370197014, 330.07328601589165, 327.9729463184578, 324.9158527745523, 323.9760818841164, 323.14087651452735, 322.3737896178028, 321.663911172029, 321.0045611824327, 320.3901490606895, 320.3901490606895, 315.5850637119911, 313.4578940929416, 311.6012881549455, 309.9179831354577, 308.3828229996809, 306.97856577487966, 305.69071760191684, 304.506684299371, 303.41574357941715, 302.4085254267155, 302.4085254267155, 299.5130526842919, 297.579653221151, 296.1325603854832, 295.06930652389786, 294.192288246979, 293.63896669923366, 291.7446755868807, 291.3741064958703, 291.04165664558946, 290.73133312027846, 290.73133312027846, 287.26101823552386, 285.79370767430225, 284.5206716144817, 283.3674717481657, 282.31611423904127, 281.35471339124933, 280.473463352692, 279.6636491246042, 278.9180956302565, 278.2304664514799, 278.2304664514799, 276.377343979151, 275.16210235354276, 274.2495243833367, 273.56099559738794, 272.98081545774204, 272.5544782644989, 272.16966773962275, 271.95128351507464, 271.7305821257236, 271.7528163636374, 271.7528163636374, 269.30310747761865, 268.4290962618955, 267.6754120360762, 266.9907401014486, 266.3648050111896, 265.79094210176305, 265.26360590475815, 264.7780851458315, 264.3305360321233, 263.9172349620233, 263.9172349620233, 261.29434259599395, 260.8010434641844, 260.4059808328643, 260.07145061949035, 259.7819100672801, 259.5267892148723, 259.2988366318434, 259.0930906548211, 258.90542134729253, 258.73292158816224, 258.73292158816224, 257.4249754844584, 256.73203571553523, 256.12395998166056, 255.57096593281995, 255.06471554330736, 254.60036928148486, 254.17357485159815, 253.78044468820886, 253.41758679692882, 253.08210630969708, 253.08210630969708, 252.10947901346552, 251.47558112344754, 250.99506255426525, 250.61807692523286, 250.2999823818782, 250.0384115557028, 249.80611963840894, 249.61528830412584, 249.43947923628787, 249.30095151544316, 249.30095151544316, 248.09109803849847, 247.57317025255603, 247.1330804832296, 246.73431879557975, 246.3686941478447, 246.0321604750011, 245.72181665968247, 245.43491635816736, 245.16916958456912, 244.92268927107202, 244.92268927107202, 244.2172275595347, 243.8280951094601, 243.50312454788403, 243.31428829093403, 243.10794645876047, 243.02961911491033, 242.89060301657167, 242.90677848874483, 242.1163774520171, 242.0510057281813, 242.0510057281813, 241.14186054616593, 240.70174476379103, 240.3249817855668, 239.98382214761108, 239.67155042086605, 239.38450534408278, 239.1199945211836, 238.87563230614168, 238.6494937027167, 238.4398436222359, 238.4398436222359, 237.90154766672762, 237.55529791143152, 237.29308266536663, 237.08777823033853, 236.91349155907784, 236.76880997669377, 236.63973220037548, 236.53064734817943, 236.43045698946815, 236.34644989632685, 236.34644989632685, 235.6201143329631, 235.27563903337884, 234.98204415283715, 234.71594890806674, 234.47198624892306, 234.2473833665707, 234.04002462045153, 233.84822770972676, 233.67053107661312, 233.50554211967005]\n",
      "\n",
      "\n",
      "Temps d'exécution de la méthode alternate avec recherche de gamma : 108.02037739753723\n"
     ]
    }
   ],
   "source": [
    "print(L_c)\n",
    "print(\"\\n\")\n",
    "print(\"Temps d'exécution de la méthode alternate avec recherche de gamma :\", fin_alternate2 - debut_alternate2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode, basée sur la recherche d'un pas optimal $\\gamma$ à chaque itération, est clairement plus efficace. On arrive à un coût de $233.50554211967005$ tandis que les autres méthodes donnent des résultats supérieurs à $340$. Pire, ce dernier algorithme s'exécute en moins de deux minutes (108 secondes ici) ; il effectue donc les 100 itérations deux fois plus vite que la méthode ``alternate``.\n",
    "\n",
    "**Néanmoins, on effectue ici en réalité 200 itérations et non pas 100 car on exécute deux fois la méthode ``projected_gradient`` par itération**. Néanmoins, même en effectuant 100 itérations, le résultat reste bien meilleur que pour la méthode avec ``projected_gradient`` (on arrive à un coût de 255 en 66 secondes).\n",
    "\n",
    "Nous pouvons résumer toutes ces informations dans le tableau suivant au bout de 100 itérations :\n",
    "\n",
    "|   | Projected method à $\\gamma$ fixé à $\\frac{1}{L_0}$ | Projected gradient avec $\\gamma$ cherché  | Alternate method à $\\gamma$ fixé à $\\frac{1}{L_0}$ | Alternate method avec $\\gamma$ cherché |\n",
    "|---|---|---|:-:|---|\n",
    "| Temps d'exécution (secondes)  | 106.0693690776825 | 129.30510926246643  | 19.240463495254517 | 108.02037739753723 |\n",
    "| Résultats  | 867.6040512602461 | 347.7974019004838 | 867.6195368103023  | 233.50554211967005  |\n",
    "\n",
    "In fine, il apparaît que fixer $\\gamma$ à $\\frac{1}{L_0}$ est une très mauvaise idée car la valeur est si faible que le coût n'arrivera quasiment pas à descendre. Après expérience, il semblerait que le fixer par exemple à $1$ améliore bien le résultat. Toutefois, nous n'avons aucune justification de ce choix, nous aurions très bien pu prendre $10$, $100$ ou $1000$... \n",
    "\n",
    "Par ailleurs, la méthode ``alternate_minimizations`` semble très clairement plus efficace que ``projected_gradient`` à la fois en terme de coût et d'exécution. \n",
    "\n",
    "En conclusion, la méthode ``alternate_minimizations`` avec recherche de $\\gamma$ avec Taylor-based line search est la méthode permettant d'atteindre le plus rapidement et avec le moins d'itérations possibles un coût très faible. Il reste alors à savoir quelle serait la condition d'arrêt permettant de savoir qu'est-ce qu'un coût \"assez\" faible.\n",
    "\n",
    "\n",
    "## Question 6.5\n",
    "\n",
    "On pourrait utiliser comme critère d'arrêt la comparaison entre le dernier et l'avant-dernier terme de l'objective value ``L``. L'algorithme s'arrêterait alors quand cette différence est inférieure à un certain $ \\epsilon $."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
